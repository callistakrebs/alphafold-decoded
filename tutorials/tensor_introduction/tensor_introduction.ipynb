{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "Run the cells below for the basic setup of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No colab environment, assuming local setup.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('No colab environment, assuming local setup.')\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # turorials folder, e.g. 'alphafold-decoded/tutorials'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "    print('Connected COLAB to Google Drive.')\n",
    "\n",
    "import os\n",
    "    \n",
    "base_folder = '../tensor_introduction'\n",
    "control_folder = f'{base_folder}/control_values'\n",
    "\n",
    "assert os.path.isdir(control_folder), 'Folder \"control_values\" not found, make sure that FOLDERNAME is set correctly.' if IN_COLAB else 'Folder \"control_values\" not found, make sure that your root folder is set correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x11edf2d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Basics\n",
    "\n",
    "A tensor is a multi-dimensional array of numerical values. A scalar (a number) is a 0th-order tensor, a vector is a 1st-order tensor, and a matrix is a 2nd-order tensor. Tensors of higher order extend this concept into additional dimensions. A colored image for example, is usually represented by a three-dimensional tensor of shape (3, H, W), where H and W are the height and width of  the image in pixels. You can think of this as three matrices stacked behind each other.\n",
    "\n",
    "In this section, we want to cover the creation of tensors, basic pointwise operations and datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor()` is the most basic function to create tensors in pytorch. It takes a nested list as an argument and converts it to a tensor. The nested list must be in block shape already, a jagged list like [[5], [5,3]] can't be converted into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "B = None\n",
    "C = None\n",
    "\n",
    "# Example:\n",
    "ex = torch.tensor([[1, 2, 3], [4, 6, 6]])\n",
    "ex_shape = ex.shape\n",
    "# ex: \n",
    "# tensor([[1, 2, 3],\n",
    "#         [4, 6, 6]])\n",
    "# ex_shape: (2, 3)\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Use torch.tensor() to create tensors of shape (5,),              # \n",
    "#       (2,3) and (1, 2, 1).                                             #\n",
    "##########################################################################\n",
    "A = torch.tensor([1, 2, 3, 4, 5])\n",
    "B = torch.tensor([[1,2,3],[4,5,6]])\n",
    "C = torch.tensor([[[1],[2]]])\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "# End of your code\n",
    "\n",
    "assert A.shape == (5,)\n",
    "assert B.shape == (2,3)\n",
    "assert C.shape == (1,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important methods that specifically create one-dimensional tensors: `torch.linspace` and `torch.arange`. `torch.linspace(start, end, steps)` creates evenly spaced values from `start` to `end`, including both `start` and `end`. `torch.arange` is usually used to create integers from 0 to N-1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "B = None\n",
    "\n",
    "# Example:\n",
    "# ex1 = torch.linspace(0, 1, steps=5)\n",
    "# ex1 is [0, 0.25, 0.5, 0.75, 1]\n",
    "# ex2 = torch.arange(3)\n",
    "# ex2 is [0, 1, 2]\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Use the methods to create the following tensors:                 #\n",
    "#   A = [-0.4, -0.2, 0, 0.2] and B = [0, 1, 2, 3, 4, 5]                  #\n",
    "##########################################################################\n",
    "\n",
    "A = torch.linspace(-0.4, 0.2, steps=4)\n",
    "B = torch.arange(6)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(A, torch.tensor([-0.4, -0.2, 0, 0.2]))\n",
    "assert torch.allclose(B, torch.tensor([0,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important methods that are often used to create tensors are `torch.zeros`, `torch.ones`, and `torch.randn` (standard normal distribution) or `torch.rand` (uniform distribution on $[0, 1)$)\n",
    "\n",
    "All of these methods take the shape of the tensor to be created as input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "B = None\n",
    "C = None\n",
    "\n",
    "# Example:\n",
    "# ex = torch.zeros((2, 2))\n",
    "# tensor([[0., 0.],\n",
    "#         [0., 0.]])\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Use the methods to create:                                       #\n",
    "# 1. zeros of shape (5, 3, 2)                                            #\n",
    "# 2. fives of shape (100,) (use `torch.ones` and scale it)               #\n",
    "# 3. standard normal distribution of shape (10, 10, 10)                  #\n",
    "##########################################################################\n",
    "\n",
    "A = torch.zeros((5,3,2))\n",
    "B = torch.ones(100) * 5\n",
    "C = torch.randn(10,10,10)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert A.shape == (5,3,2) and torch.allclose(A, torch.tensor([0.0]))\n",
    "assert B.shape == (100,) and torch.allclose(B, torch.tensor([5.0]))\n",
    "assert C.shape == (10,10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use most basic operations like +, -, *, /, >, <, == on tensors of the same shape or, or on tensors and scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "B = None\n",
    "C = None\n",
    "\n",
    "# Example:\n",
    "# ex = torch.tensor([-1, 2, -3])\n",
    "# ex_bool = ex < 0\n",
    "# Result: torch.tensor([True, False, True])\n",
    "\n",
    "##########################################################################\n",
    "# DONE:                                                                  #\n",
    "# 1. Try out + or *: Create a tensor A where each element is 384, by     # \n",
    "#     creating to tensors A1 and A2 first (choose their values)          #\n",
    "#     followed by A = A1 * A2, or A = A1 + A2. Choose any shape.         #\n",
    "# 2. Create a linspace with 6 values between -0.6 and 0.3 and            #\n",
    "#     compute which elements are smaller than -0.1.                      #\n",
    "# 3. sample normally distributed values of shape (100,100,100) with      #\n",
    "#     mean 0.5 and standard deviation 3.0 by scaling and adding          #\n",
    "#     to the standard normal distribution `torch.randn(size)`.           #\n",
    "##########################################################################\n",
    "\n",
    "A1 = torch.ones(5)\n",
    "A2 = torch.ones(5) * 384\n",
    "A = A1 * A2\n",
    "\n",
    "B = torch.linspace(-0.6, 0.3, 6) < -0.1\n",
    "\n",
    "C = torch.randn(100,100,100) * 3 + 0.5\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(A, torch.tensor([384.0]))\n",
    "assert torch.all(B == torch.tensor([True, True, True, False, False, False]))\n",
    "assert abs(C.std()-3.0)<0.05 and abs(C.mean() - 0.5) < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can have different datatypes. The most important ones for our project are `long` (`torch.int64`), `float` (`torch.float32`) and `bool` (`torch.bool`). We'll sometimes use `double` (`torch.float64`) for checks on your implementation. It's for floating point numbers like `float`, but with higher precision and more memory consumption.\n",
    "\n",
    "You can cast a tensors with a syntax like `A.long()` or `A.to(torch.int64)`. \n",
    "\n",
    "When converting from float to long (or to int), the decimal part is simply dropped. When converting from bool to float, True is converted to 1.0 and False to 0.0. When going from float or long to bool, non-zero values are interpreted as True, while zero values are interpreted as False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "entry_count = None\n",
    "\n",
    "##########################################################################\n",
    "# DONE:                                                                  #\n",
    "# 1. Create linspace(-2, 3, 5) and cast it to long.                      #\n",
    "# 2. Sample uniform values of shape (100, 200, 50) and count how many    #\n",
    "#     elements are smaller than 0.2. You can use torch.sum() to sum      #\n",
    "#     all elements in a tensor.                                          #\n",
    "##########################################################################\n",
    "\n",
    "A = torch.linspace(-2, 3, 5).long()\n",
    "entry_count = torch.sum(torch.rand(100,200,50) < 0.2)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.all(A == torch.tensor([-2, 0, 0, 1, 3]))\n",
    "assert abs(entry_count/(100*200*50) - 0.2) < 5e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Shapes\n",
    "The shape of a tensor is a tuple consisting of the size of each dimension. Tensors can be reshaped without changing the underlying data using functions like `torch.reshape()`, `torch.view()`, and `torch.transpose()`. `torch.reshape` and `torch.view` take the new shape of the tensor as argument, while `torch.transpose` takes the indices of the two dimension that you want to swap.\n",
    "\n",
    "The difference between `view()`and `reshape()` is subtle. `view()` will always try to use the same underlying data and will throw an error if this is not possible, while `reshape()` will try to create a view, but will copy the data if it's not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.arange` and the new commands to create the desired outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = None\n",
    "B = None\n",
    "C = None\n",
    "correct_A = torch.tensor([\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9, 10, 11],\n",
    "])\n",
    "correct_B = torch.tensor([\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "    [8, 9, 10, 11],\n",
    "])\n",
    "correct_C = torch.tensor([\n",
    "    [0, 4, 8],\n",
    "    [1,  5, 9],\n",
    "    [2, 6, 10],\n",
    "    [3, 7, 11],\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Use reshape/view and transpose to construct the tensors above    #\n",
    "#        from a linear torch.arange tensor.                              #\n",
    "##########################################################################\n",
    "\n",
    "A = torch.arange(12).reshape(4,3)\n",
    "B = torch.arange(12).reshape(3,4)\n",
    "C = torch.arange(12).reshape(3,4).transpose(0,1)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.all(A == correct_A)\n",
    "assert torch.all(B == correct_B)\n",
    "assert torch.all(C == correct_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "Tensors support Python-like indexing to access elements, as well as more advanced slicing techniques to select ranges and sub-tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Value Access: Use square brackets [] with an integer index to access a single element. The index starts from 0, where 0 refers to the first element.\n",
    "\n",
    "Slicing a Sub-tensor: Extract a contiguous sub-tensor using a colon : to separate the starting and ending indices (exclusive of the ending index).  Omitting the starting index defaults to the beginning, and omitting the ending index defaults to the end.\n",
    "\n",
    "Negative Indexing: Negative indices are supported, allowing you to index from the end of a dimension. For example, tensor[-1] refers to the last element in the tensor.\n",
    "\n",
    "Newaxis (Adding Dimensions): Use None or newaxis to insert a new dimension of size 1 at a specific position in the indexing expression. This is useful for broadcasting operations.\n",
    "\n",
    "Boolean Indexing:  You can use a boolean tensor (having the same shape as the tensor you want to index) as an index.  Elements where the boolean tensor has True are selected, and those corresponding to False are omitted in the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these indexing techniques to get the following values from the provided array:\n",
    "* element at row with index 1, column with index 2 (use .item() to get the value instead of a 0-dimensional tensor)\n",
    "* all elements in the row with index 1\n",
    "* the last column (use negative indexing)\n",
    "* the same array, but with an additional dimension 1 introduced between the row and column dimension and without the third column.\n",
    "* a boolean mask representing all the values equal to 2\n",
    "* a copy of the array, where all 2-values are set to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([\n",
    "        [3, 0, 1],\n",
    "        [1, 1, 2],\n",
    "        [2, 1, 0],\n",
    "        [1, 1, 1]])\n",
    "\n",
    "single_value = None\n",
    "row = None\n",
    "column = None\n",
    "unsqueezed = None\n",
    "mask = None\n",
    "no_2s = None\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Use indexing to retrieve the elements described above.           #\n",
    "##########################################################################\n",
    "\n",
    "single_value = A[1,2].item()\n",
    "row = A[1]\n",
    "column= A[:,-1]\n",
    "unsqueezed = A[:,0:2].unsqueeze(1)\n",
    "mask = A == 2\n",
    "no_2s = A.clone()\n",
    "no_2s[mask] = -1\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert single_value == 2\n",
    "assert torch.all(row == torch.tensor([1,1,2]))\n",
    "assert torch.all(column == torch.tensor([1,2,0,1]))\n",
    "assert torch.all(unsqueezed == torch.tensor([[[3, 0]],[[1, 1]],[[2, 1]],[[1, 1]]]))\n",
    "assert torch.all(mask == torch.tensor([[False, False, False],[False, False,  True],[ True, False, False],[False, False, False]]))\n",
    "assert torch.all(no_2s == torch.tensor([[ 3,  0,  1],[ 1,  1, -1],[-1,  1,  0],[ 1,  1,  1]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting  \n",
    "Broadcasting  allows element-wise operations between tensors with compatible shapes.  Rules govern how tensor dimensions are expanded to facilitate these operations.\n",
    "\n",
    "The rules are the following: \n",
    "* dimensions are aligned starting with the right-most dimension\n",
    "* all dimensions must each agree or be 1 for one of the tensors\n",
    "* if the number of dimensions is smaller for one of the tensors, 1-dimensions are prepended until the dimensions match\n",
    "\n",
    "When one of the dimensions is 1 while the other is not, the tensor is copied along this dimension to match the shape of the other tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(12).reshape(4,3)\n",
    "added_row = None\n",
    "added_column = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Use broadcasting to:                                             #\n",
    "# 1. Compute a matrix, where the first row of A is added to all rows     #\n",
    "#     of A (including the first).                                        #\n",
    "# 2. Compute a matrix, where the first column of A is added to all       #\n",
    "#     other columns of A (excluding the first).                          #\n",
    "#                                                                        #\n",
    "# You might need to introduce new 1-dimensions to fullfill the           #\n",
    "#  requirements for broadcasting.                                        #\n",
    "##########################################################################\n",
    "\n",
    "added_row = A + A[0].reshape(1,3)\n",
    "added_column = torch.cat((A[:,0].reshape(4,1), A[:,1:] + A[:,0].reshape(4,1)), dim=1)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.all(added_row==torch.tensor([[ 0,  2,  4],[ 3,  5,  7],[ 6,  8, 10],[ 9, 11, 13]]))\n",
    "assert torch.all(added_column == torch.tensor([[ 0,  1,  2],[ 3,  7,  8],[6, 13, 14],[9, 19, 20]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting often plays a special role when performing 'each-with-each' operations, like the outer sum of two vectors. Use broadcasting two compute the outer sum, i.e. a matrix where the entry [i, j] is the sum of v[i] and w[j] by first introducing new 1-dimensions at the correct positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.arange(3)\n",
    "w = torch.arange(4)\n",
    "outer_sum = None\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Compute the outer sum of v and w.                                #\n",
    "##########################################################################\n",
    "\n",
    "outer_sum = v.reshape(3,1) + w\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "assert torch.all(outer_sum==torch.tensor([[0, 1, 2, 3],[1, 2, 3, 4],[2, 3, 4, 5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations in pytorch don't allow implicit broadcasting, and we need to broadcast the values explicitly with `torch.broadcast_to()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/94553741.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/broadcast_explicit.pt')\n"
     ]
    }
   ],
   "source": [
    "matrix_a = torch.arange(6)\n",
    "matrix_b = torch.arange(6*4*3).reshape(4, 6, 3)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/broadcast_explicit.pt')\n",
    "broadcasted_a = None\n",
    "##########################################################################\n",
    "# TODO: Use torch.broadcast_to to broadcast matrix_a to the              #\n",
    "#   shape of matrix_b.                                                   #\n",
    "#                                                                        #\n",
    "# You might need to introduce new 1-dimensions to fullfill the           #\n",
    "#  requirements for broadcasting.                                        #\n",
    "##########################################################################\n",
    "\n",
    "broadcasted_a = matrix_a.reshape(6,1).broadcast_to(matrix_b.shape)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(broadcasted_a, expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening and Unflattening\n",
    "\n",
    "- Flattening refers to the process of reducing the number of dimensions in a tensor. This involves combining multiple dimensions into a single dimension. PyTorch provides functions such as torch.flatten(), torch.ravel(), or tensor.view(-1) for this operation. \n",
    "\n",
    "- Unflattening is the reverse process of restoring a higher-dimensional shape from a tensor with a reduced number of dimensions. This typically involves using  torch.reshape() or tensor.view().\n",
    "\n",
    "- The Importance of Dimension Order: The order in which dimensions are arranged matters significantly during both flattening and unflattening. Flattening follows a specific convention (by default row-wise in pytorch), and it's crucial to know this order when unflattening to reconstruct the original shape correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Incorrectly unflattened: \n",
      "tensor([[ 0,  4,  8,  1],\n",
      "        [ 5,  9,  2,  6],\n",
      "        [10,  3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.arange(12).reshape(3, 4)\n",
    "flattened_row_wise = None\n",
    "flattened_column_wise = None\n",
    "incorrect_unflatten = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Use flatten, reshape and transpose to                            #\n",
    "# 1. flatten the matrix row-wise.                                        #\n",
    "# 2. flatten the matrix column-wise.                                     #\n",
    "# 3. unflatten the column-wise flattened matrix, as if                   #\n",
    "#     it was flattened row-wise.                                         #\n",
    "##########################################################################\n",
    "\n",
    "flattened_row_wise = matrix.flatten()\n",
    "flattened_column_wise = matrix.transpose(0,1).flatten()\n",
    "incorrect_unflatten = flattened_column_wise.reshape(3,4)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.all(flattened_row_wise == torch.arange(12))\n",
    "assert torch.all(flattened_column_wise == torch.tensor([0,4,8,1,5,9,2,6,10,3,7,11]))\n",
    "assert torch.all(incorrect_unflatten==torch.tensor([[0, 4, 8, 1],[5, 9, 2, 6],[ 10, 3, 7, 11]]))\n",
    "\n",
    "print('Original matrix:')\n",
    "print(matrix)\n",
    "print('Incorrectly unflattened: ')\n",
    "print(incorrect_unflatten)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Tensor Reductions\n",
    "\n",
    "PyTorch offers a variety of functions that perform reduction operations on tensors. These operations collapse a tensor along specified dimensions, often resulting in a lower-dimensional tensor or a single scalar value. Examples include calculating the sum, mean, standard deviation (std), or finding the norm of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(12).reshape(3, 4)\n",
    "sum_all = None\n",
    "mean_columns = None\n",
    "std_all = None\n",
    "l2_rows = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Use the following reduction operations on the provided tensor.   #\n",
    "# 1. Calculate the sum of all elements (use torch.sum()).                #\n",
    "# 2. Calculate the mean of each column (use torch.mean()).               #\n",
    "# 3. Calculate the standard deviation of all elements (use torch.std()). #\n",
    "# 4. Calculate the L2 norm of each row (use torch.linalg.vector_norm()). #\n",
    "# 5. Try keeping the original dimensions using the `keepdim` argument    #\n",
    "#     in the reduction calls. How do the resulting tensors differ?       #\n",
    "#                                                                        #\n",
    "# You might need to cast the data type for some of the operations and    #\n",
    "#  use .item() to get numbers instead of 0-dimensional tensors.          #\n",
    "##########################################################################\n",
    "\n",
    "sum_all= tensor.sum().item()\n",
    "mean_columns = tensor.float().mean(dim=0)\n",
    "std_all = tensor.float().std().item()\n",
    "l2_rows = torch.linalg.vector_norm(tensor.float(), ord=2, dim=1)\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert abs(sum_all-66)<1e-3\n",
    "assert torch.allclose(mean_columns, torch.tensor([4.0,5.0,6.0,7.0]))\n",
    "assert abs(std_all-3.60555) < 1e-3\n",
    "assert torch.allclose(l2_rows, torch.tensor([ 3.7417, 11.2250, 19.1311]), rtol=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: The Power of torch.einsum\n",
    "\n",
    "Einstein Summation (einsum) is a concise and powerful notation for specifying complex tensor operations. It offers a flexible way to perform operations like matrix multiplication, dot products, batch calculations, outer products, reductions, and much more. \n",
    "\n",
    "Most calculations in our AlphaFold implementation that are not reshaping or transposing axis will be done with torch.einsum.\n",
    "\n",
    "The Core Idea: torch.einsum() takes an equation string and input tensors as arguments. The equation string dictates how to manipulate and combine the dimensions of the input tensors to produce the desired output tensor. \n",
    "\n",
    "Simply designing the equation so the output dimensions match up is often a good indicator that your expression is on the right track.\n",
    "\n",
    "Note:\n",
    "Example for matrix multiplication: $M_{ij} = \\sum{A_{ij}B_{kj}}$ : einsum subscript would be 'ik,kj->i,j' meaning that the element at position i,j in the output matrix is equal to the sum for product of Row i in A with Column k in B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 25,  28,  31,  34,  37],\n",
      "        [ 70,  82,  94, 106, 118]])\n",
      "tensor([[ 0,  1,  4],\n",
      "        [ 0,  4, 10]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Matrix Multiplication  \n",
    "matrix_a = torch.arange(6).reshape(2, 3)\n",
    "matrix_b = torch.arange(15).reshape(3, 5)\n",
    "result = torch.einsum('ik,kj->ij', matrix_a, matrix_b)\n",
    "print(result)\n",
    "\n",
    "# Example 2: Element-wise operations\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(3)\n",
    "result = torch.einsum('ij,j->ij', a, b)  # Broadcast and multiply\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ellipsis (...) in einsum: The ellipsis (...) is a powerful wildcard notation in torch.einsum  representing one or more unspecified dimensions. It allows you to write einsum expressions that work for tensors with an arbitrary number of prepended batch dimensions.\n",
    "\n",
    "How it simplifies einsum: Without ellipsis, you'd need to explicitly specify every batch dimension in the equation string, which can become cumbersome for tensors with complex shapes. Ellipsis helps write concise and general expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/339908859.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  matrix_a = torch.load(f'{control_folder}/einsum_bmm_a.pt')\n",
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/339908859.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  matrix_b = torch.load(f'{control_folder}/einsum_bmm_b.pt')\n",
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/339908859.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/einsum_bmm_result.pt')\n"
     ]
    }
   ],
   "source": [
    "# Batched Matrix Multiplication\n",
    "# matrix_a has shape (5, 3, 2, 3)\n",
    "# matrix_b has shape (5, 3, 3, 5)\n",
    "matrix_a = torch.load(f'{control_folder}/einsum_bmm_a.pt')\n",
    "matrix_b = torch.load(f'{control_folder}/einsum_bmm_b.pt')\n",
    "result = None\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Perform batched matrix multiplication using torch.einsum().      #\n",
    "#  The result should be of shape (5, 3, 2, 5).                           #\n",
    "#  Try do to it both with and without the ellipsis operator.             #\n",
    "##########################################################################\n",
    "\n",
    "result = torch.einsum('...ik,...kj->...ij', matrix_a, matrix_b)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/einsum_bmm_result.pt')\n",
    "assert torch.allclose(result, expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction: (Sum over a dimension)\n",
    "tensor = torch.arange(24).reshape(2, 3, 4)\n",
    "expected_result = torch.sum(tensor, dim=1)\n",
    "result = None\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Compute the equivalent of torch.sum with torch.einsum.           #\n",
    "##########################################################################\n",
    "\n",
    "result = torch.einsum('...ij->...j', tensor)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(expected_result, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll move on to compute a little more complicated einsum expressions, starting with `scaled_dot_product_attention`, one of the core primitives for AlphaFold. \n",
    "\n",
    "We'll go into more detail on this when we implement MultiHeadAttention later in this series. For now, this is the computation we'll perform:\n",
    "\n",
    "Inputs:\n",
    "- q query vectors of dimension c (represented as a matrix Q of shape (q, c))\n",
    "- k key vectors of dimension c (represented as a matrix K of shape (k, c))\n",
    "- k value vectors of dimension d (represented as matrix V of shape (k, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32]) torch.Size([16, 32]) torch.Size([16, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1254564384.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Q = torch.load(f'{control_folder}/att_Q.pt')\n",
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1254564384.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  K = torch.load(f'{control_folder}/att_K.pt')\n",
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1254564384.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  V = torch.load(f'{control_folder}/att_V.pt')\n"
     ]
    }
   ],
   "source": [
    "Q = torch.load(f'{control_folder}/att_Q.pt')\n",
    "K = torch.load(f'{control_folder}/att_K.pt')\n",
    "V = torch.load(f'{control_folder}/att_V.pt')\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compute the agreement of Q and K. This is done by computing the dot product of each query vector with each key vector. The result is divided by the square root of c, so that the expected values are independent of c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/2825027460.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/att_raw_attn.pt')\n"
     ]
    }
   ],
   "source": [
    "raw_attn = None\n",
    "expected_result = torch.load(f'{control_folder}/att_raw_attn.pt')\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Implement the scaled dot product.                                #\n",
    "#   The result should be of shape (q, k).                                #\n",
    "##########################################################################\n",
    "\n",
    "raw_attn = torch.einsum('ik,lk->il',Q,K) / math.sqrt(Q.shape[-1])\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "assert torch.allclose(raw_attn, expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These raw attention scores are mapped through the softmax function along the key dimension, so that for every query, the values of all keys sum to 1.\n",
    "\n",
    "The softmax function looks like this:\n",
    "\n",
    "$softmax(x)_i =  \\frac{\\exp(x_i)}{\\sum_{j=1}^{K} \\exp(x_j)}$\n",
    "\n",
    "It's main properties are the following:\n",
    "- Individual entries sum to 1, so the result can be interpreted as a probability distribution\n",
    "- If one value was significantly larger (let's say bigger by 6) than the others, softmax will be close to 1 for this value and close to 0 for the others, somewhat like the max  function\n",
    "- Unlike a hard max function, if the biggest values are close to each other, they will have similar scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/228877652.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/att_normalized_attn.pt')\n"
     ]
    }
   ],
   "source": [
    "attn = None\n",
    "expected_result = torch.load(f'{control_folder}/att_normalized_attn.pt')\n",
    "##########################################################################\n",
    "# DONE: Use torch.softmax to convert the scores for all queries          #\n",
    "#   to a probability distribution.                                       #\n",
    "##########################################################################\n",
    "\n",
    "attn = torch.softmax(raw_attn, dim=1)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(attn, expected_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, for each query vector, we'll scale the value vectors by the corresponding attention weights and sum these value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1364318531.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/att_result.pt')\n"
     ]
    }
   ],
   "source": [
    "result = None\n",
    "expected_result = torch.load(f'{control_folder}/att_result.pt')\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Use torch.einsum to scale the value vectors by the               #\n",
    "#  corresponding weights and add them up.                                #\n",
    "#  The result should be of shape (q, d).                                 #\n",
    "##########################################################################\n",
    "\n",
    "result = torch.einsum('ij,jk->ik', attn, V)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "assert torch.allclose(result, expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: A Practical Example with MNIST\n",
    "\n",
    "In this section, we'll put your newfound tensor skills to practice with a simple image classification task. We'll use PyTorch to load the classic MNIST dataset of handwritten digits, compute average images for each digit, and build a rudimentary classifier based on these averages. While this classifier is basic, it demonstrates how core tensor operations can be used in a real-world machine learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell loads in the dataset. Its content isn't too important to us right now, so feel free to skip over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load and Prepare MNIST Dataset ---\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])  \n",
    "\n",
    "# Download and split into training and validation sets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "validation_set = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Load all training and validation images and labels\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set))\n",
    "\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "validation_images, validation_labels = next(iter(validation_loader))\n",
    "\n",
    "train_images = train_images.double()\n",
    "validation_images = validation_images.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, we loaded in the images and labels as two big tensors, split into training and validation sets.\n",
    "The pixel values are normalized so that they follow a normal distribution with mean 0 and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation and training images shape:\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n",
      "\n",
      "Mean: -0.0001282926419238658\n",
      "Std: 1.0000254196938925\n",
      "\n",
      "Train labels shape:\n",
      "torch.Size([60000])\n",
      "Train label values:\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print('Validation and training images shape:')\n",
    "print(validation_images.shape)\n",
    "print(train_images.shape)\n",
    "print('')\n",
    "\n",
    "print(f'Mean: {train_images.mean()}')\n",
    "print(f'Std: {train_images.std()}')\n",
    "print('')\n",
    "\n",
    "print('Train labels shape:')\n",
    "print(train_labels.shape)\n",
    "print('Train label values:')\n",
    "print(torch.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the images are of shape (N, 1, H, W). The one represents the channel. If this were an RGB image, the dimension would be three. However, we don't need this dimension in our case, so we will squeeze the tensors to get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.squeeze()\n",
    "validation_images = validation_images.squeeze()\n",
    "print(train_images.shape)\n",
    "print(validation_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, nrows=None, ncols=None):\n",
    "    if ncols is None:\n",
    "        ncols = min(len(images), 5)\n",
    "    if nrows is None:\n",
    "        nrows = math.ceil(len(images) / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 8))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i].squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(train_images[:10,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Averages\n",
    "We want to put the images into 10 groups by their labels. There is a number of ways we could do this:\n",
    "1) Use argsort to sort the labels in ascending order, apply the permutation calculated while sorting to the images, count the number of occurences for each label (with torch.bincount) and split the sorted images accordingly.\n",
    "\n",
    "2) Generate boolean masks for each label and use them to select the correct images (this is probably the easiest one to implement).\n",
    "\n",
    "3) Get the indices of all labels with value $i$, then use these indices with array indexing to get the corresponding images.\n",
    "\n",
    "Choose a method, implement it and use it to get a list of tensors, where the 0th element of the list are the images with label 0 and so on (Could we also stack these tensors to a new tensor with additional dimension 10? If not, why so?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_images = []\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Group the images by their label using one of the methods above.  #\n",
    "##########################################################################\n",
    "\n",
    "permutation = torch.argsort(train_labels)\n",
    "train_images = train_images[permutation]\n",
    "\n",
    "split_train_images = torch.split(train_images, torch.bincount(train_labels).tolist())\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert all([len(a) == l for a,l in zip(split_train_images, [5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the image rows should show 0, 1 and 2 accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    plot_images(split_train_images[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/4054767124.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/mnist_mean_images.pt')\n"
     ]
    }
   ],
   "source": [
    "mean_images = None\n",
    "expected_result = torch.load(f'{control_folder}/mnist_mean_images.pt')\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Calculate the mean of each image group in the split train images.# \n",
    "#       Stack the mean images with torch.stack, so that the              #\n",
    "#       resulting tensor has shape (10, 28, 28).                         #\n",
    "##########################################################################\n",
    "\n",
    "mean_images = torch.stack([group.mean(dim=0) for group in split_train_images])\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(expected_result, mean_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the mean images. They should all be blurry versions of the numbers they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(mean_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use these templates on the validation images and calculate their agreement. In this case, agreement simply means multiplying every pixel in the templates against every pixel from the validation image, then summing them up.\n",
    "To make sure this is a fair metric, we will first normalize the mean_images, so that they have mean 0 and std 1 individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1137509067.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/mnist_normalized_templates.pt')\n"
     ]
    }
   ],
   "source": [
    "template_images = None\n",
    "expected_result = torch.load(f'{control_folder}/mnist_normalized_templates.pt')\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Calculate mean and std for each mean_image individually.         #\n",
    "#  You can do this without a loop by choosing the dim argument           #\n",
    "#  from torch.mean(...) and torch.std(...) correctly.                    #\n",
    "#  Then, use the mean and  std to normalize the template images.         #\n",
    "#  keepdim might be helpful here (why?).                                 #\n",
    "##########################################################################\n",
    "\n",
    "mean = torch.mean(mean_images, dim=(1,2), keepdim=True)\n",
    "std = torch.std(mean_images, dim=(1,2), keepdim=True)\n",
    "\n",
    "template_images = (mean_images - mean) / std\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(template_images, expected_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 28, 28])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the agreement of the validation images with the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1910052503.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/mnist_agreement.pt')\n"
     ]
    }
   ],
   "source": [
    "agreement = None\n",
    "expected_result = torch.load(f'{control_folder}/mnist_agreement.pt')\n",
    "\n",
    "##########################################################################\n",
    "# DONE: Calculate the agreement of the template_images with the          #\n",
    "#  validation_images as a dot-product. You can use torch.einsum          #\n",
    "#  to do this effectively in one line.                                   #\n",
    "#                                                                        #\n",
    "#  template_images has shape (10, 28, 28),                               #\n",
    "#  validation_images has shape (N_val, 28, 28) and                       #\n",
    "#  the output shape should be (N_val, 10).                               #\n",
    "##########################################################################\n",
    "\n",
    "agreement = torch.einsum('ijk,ljk->il', validation_images, template_images)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "print(agreement.shape)\n",
    "assert torch.allclose(agreement, expected_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/xkz34clj6158f543g3k49h3m0000gn/T/ipykernel_9193/1920408382.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  expected_result = torch.load(f'{control_folder}/mnist_validation_guess.pt')\n"
     ]
    }
   ],
   "source": [
    "validation_guess = None\n",
    "expected_result = torch.load(f'{control_folder}/mnist_validation_guess.pt')\n",
    "\n",
    "##########################################################################\n",
    "# DONE: assign each validation image to the class label with             #\n",
    "#  the biggest score. You can use torch.argmax for this.                 #\n",
    "##########################################################################\n",
    "\n",
    "validation_guess = torch.argmax(agreement, dim=1)\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert torch.allclose(validation_guess, expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's check how good our classification was. Calculate how many times our guess agreed with the label on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8208)\n"
     ]
    }
   ],
   "source": [
    "accuracy = None\n",
    "##########################################################################\n",
    "# TODO: Compare our validation_guess with the validation_labels and      #\n",
    "#  calculate how many times they were equal on average.                  #\n",
    "##########################################################################\n",
    "\n",
    "accuracy = (validation_guess == validation_labels).float().mean()\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an accuracy above 80% and quite a strong baseline to beat. Our method of calculating average images and using them as filters works really well for this use-case. This is in part because the MNIST-Dataset entries are size-normalized and centered, which makes them a good target for this approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook provided a solid introduction to tensor manipulations in PyTorch. Remember, learning is an ongoing process, and even experienced developers continually expand their knowledge base. The skills you practiced here will be valuable as you encounter new challenges.\n",
    "\n",
    "Next, we'll explore the fundamentals of machine learning by building a feed-forward classifier for handwritten digits. We'll introduce the concept of feature hierarchies, where multiple layers learn to identify increasingly complex patterns. You'll also learn gradient descent – a cornerstone technique for optimizing machine learning models.\n",
    "\n",
    "Following that, we'll delve into attention mechanisms (you've already seen the basics with scaled dot-product attention!) and apply them to construct a key AlphaFold component: MultiHeadAttention.\n",
    "\n",
    "Finally, we'll start implementing AlphaFold's core architecture. Congratulations on making it so far! Stay tuned as we embark on this exciting journey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
